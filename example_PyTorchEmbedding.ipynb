{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example-PyTorchEmbedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+Vb1zAjg/LPLz7nWXjqVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JJnotJimmyJohn/CodeAlongNLPwithPyTorch/blob/master/example_PyTorchEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvjR5APuUHBc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKbDdJmoUkfq"
      },
      "source": [
        "# Toy Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5W6I_H3Uffj"
      },
      "source": [
        "embeds = nn.Embedding(2,5)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVwg3Zo7UfkS",
        "outputId": "e87de6bf-2c6c-4c7c-989a-deff95786ac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "embeds(torch.tensor(1,dtype=torch.long))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.2899, -0.3460,  0.7215, -0.4418,  0.6033],\n",
              "       grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6X3Be1wUfpj"
      },
      "source": [
        "# N-gram Language modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyrZKZA3UfvA"
      },
      "source": [
        "from collections import namedtuple"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2Otv-zMUf0K"
      },
      "source": [
        "Config = namedtuple('Config',['context_size','embedding_dim'])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRoLO6k7Uf5D"
      },
      "source": [
        "args = Config(2,10)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muDNdMryyljk",
        "outputId": "33e8af29-398c-46c4-b0c3-58ff756d7c19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "args.context_size"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONpPp-4ryq-w",
        "outputId": "8d45f4c8-3315-40ad-b7b2-c147e45bea29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "args.embedding_dim"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbBDpK_9Uf9m"
      },
      "source": [
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y8Q3fqOUgCh",
        "outputId": "8d8dffd0-a3de-46c5-bbec-84aabcc412a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "ngrams = [(test_sentence[i:i+args.context_size],test_sentence[i+args.context_size]) for i in range(len(test_sentence)-args.context_size)]\n",
        "ngrams[:3]"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['When', 'forty'], 'winters'),\n",
              " (['forty', 'winters'], 'shall'),\n",
              " (['winters', 'shall'], 'besiege')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvHtxyIVUgHQ"
      },
      "source": [
        "# assign each word a index, later we will use that index to locate the embedding\n",
        "words = list(set(test_sentence))\n",
        "vocab_size = len(words)\n",
        "word_to_idx = {word:num for word, num in zip(words,range(len(words)))}"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUiJNfYb3FaS",
        "outputId": "b4048d66-99f5-4a59-c419-bdcc0aea653e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(word_to_idx)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'cold.': 0, 'much': 1, 'gazed': 2, 'Then': 3, 'the': 4, 'my': 5, 'more': 6, 'thine': 7, 'days;': 8, 'Were': 9, 'to': 10, 'Where': 11, 'and': 12, 'child': 13, 'trenches': 14, \"beauty's\": 15, 'shame,': 16, 'Thy': 17, 'dig': 18, \"'This\": 19, 'praise.': 20, 'all-eating': 21, 'treasure': 22, 'Proving': 23, 'This': 24, 'succession': 25, 'livery': 26, 'an': 27, \"excuse,'\": 28, 'see': 29, 'were': 30, \"feel'st\": 31, 'when': 32, 'proud': 33, 'besiege': 34, 'Shall': 35, 'held:': 36, 'weed': 37, 'be': 38, 'eyes,': 39, 'art': 40, 'all': 41, \"deserv'd\": 42, 'old': 43, 'new': 44, 'mine': 45, 'it': 46, 'couldst': 47, 'answer': 48, 'now,': 49, 'thou': 50, 'his': 51, 'shall': 52, 'forty': 53, 'praise': 54, \"youth's\": 55, 'so': 56, 'small': 57, 'use,': 58, 'warm': 59, 'made': 60, 'within': 61, 'own': 62, 'worth': 63, 'Will': 64, 'of': 65, 'fair': 66, 'How': 67, 'deep': 68, 'thriftless': 69, 'blood': 70, 'on': 71, 'thy': 72, 'old,': 73, 'by': 74, 'And': 75, 'sum': 76, 'being': 77, 'lies,': 78, 'count,': 79, 'If': 80, 'sunken': 81, 'asked,': 82, 'thine!': 83, 'brow,': 84, \"totter'd\": 85, 'where': 86, 'lusty': 87, 'in': 88, 'beauty': 89, 'To': 90, 'say,': 91, 'winters': 92, 'When': 93, 'a': 94, 'make': 95, 'field,': 96}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXqwWTaA3pqq"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsLfHxge3pwQ"
      },
      "source": [
        "class NGramLM(nn.Module):\n",
        "  def __init__(self,vocab_size,embedding_dim,context_size):\n",
        "    super(NGramLM,self).__init__()\n",
        "    # first get an embedding layer\n",
        "    self.embeddings = nn.Embedding(vocab_size,embedding_dim)\n",
        "    # input size = context words size * embedding dimension\n",
        "    self.linear1 = nn.Linear(embedding_dim*context_size,128)\n",
        "    # input size = last output size\n",
        "    # need vocab_size vector to guess which word that is\n",
        "    self.linear2 = nn.Linear(128,vocab_size)\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    embeds = self.embeddings(inputs).view((1,-1))\n",
        "    out = F.relu(self.linear1(embeds))\n",
        "    out = self.linear2(out)\n",
        "    log_probs = F.log_softmax(out,dim=1)\n",
        "    return log_probs"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMslUMpc3p17"
      },
      "source": [
        "# set up loss function and optimizer\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "model = NGramLM(vocab_size,args.embedding_dim,args.context_size)\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.001)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WzNeEuE9jZ1",
        "outputId": "7b53947e-dad9-4881-d6e2-32ff09643e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.embeddings(torch.tensor(2,dtype=torch.long))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.8326,  0.3169,  0.4066,  0.9902,  0.6498,  0.5291, -0.8644,  0.3723,\n",
              "        -0.3940,  0.0707], grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLSdbHW63p7d",
        "outputId": "8e66074b-4adf-43ab-a098-aa6f9cb5b430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for epoch in range(10):\n",
        "  total_loss = 0\n",
        "  for context, target in ngrams:\n",
        "\n",
        "    embed_idxs = torch.tensor([word_to_idx[word] for word in context],dtype=torch.long)\n",
        "    model.zero_grad()\n",
        "    \n",
        "    log_probs = model(embed_idxs)\n",
        "\n",
        "    loss = loss_function(log_probs,torch.tensor([word_to_idx[target]], dtype=torch.long))\n",
        "\n",
        "    # backward and update parameters\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss+=loss.item()\n",
        "  losses.append(total_loss)\n",
        "print(losses)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[519.6402027606964, 517.3343765735626, 515.0431771278381, 512.7640767097473, 510.49564933776855, 508.2377498149872, 505.98920369148254, 503.75186586380005, 501.52382731437683, 499.30370926856995]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6bjnyys3qIV",
        "outputId": "0f8b9f9d-944d-4977-b6cc-5957b07196db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(1)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}